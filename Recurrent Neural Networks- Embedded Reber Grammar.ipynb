{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Example: Predict Embedded Reber Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reber grammar is a definition how to create strings. Allowed values for the next character depend on the previous character.\n",
    "That makes them a good learning problem for models that need memory of past events.\n",
    "\n",
    "<img src=\"data/Artificial_grammar_learning_example.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(17)\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=\"TPXVS\"\n",
    "\n",
    "prev_to_next_mapping={\n",
    "        \"T\":(\"P\",\"T\"),\n",
    "        \"TP\":(\"P\",\"T\"),\n",
    "        \"PP\":(\"P\",\"T\"),\n",
    "        \"PT\":(\"S\",\"X\"),\n",
    "        \"TX\":(\"X\",\"V\"),\n",
    "        \"TT\":(\"X\",\"S\"),\n",
    "        \"XX\":(\"X\",\"V\"),\n",
    "        \"V\":(\"X\",\"V\"),\n",
    "        \"VX\":(\"X\",\"V\"),\n",
    "        \"VV\":(\"P\",\"S\"),\n",
    "        \"VP\":(\"S\",\"S\"),\n",
    "        \"XV\":(\"P\",\"S\")\n",
    "        }\n",
    "\n",
    "def create_reber_string():\n",
    "    reber_string='V' if np.random.random()<0.5 else 'T'\n",
    "    while reber_string[-1] !=\"S\":\n",
    "        sampled_option=random.random()<0.5\n",
    "        #print(prev_to_next_mapping[reber_string[-2:]],sampled_option)\n",
    "        reber_string+=prev_to_next_mapping[reber_string[-2:]][sampled_option]\n",
    "    return reber_string\n",
    "\n",
    "def embedded_reber_string(max_size=30):\n",
    "    if max_size<5:\n",
    "        raise Exception(\"Embedded reber strings are at least of size 5\")\n",
    "    start='V' if np.random.random()<0.5 else 'T'\n",
    "    middle=create_reber_string()\n",
    "    while len(middle)>max_size-2:\n",
    "        middle=create_reber_string()\n",
    "    end=start\n",
    "    return start+middle+end\n",
    "\n",
    "def not_reber_string(max_size=30,vocab:set=set(vocabulary)):\n",
    "    correct=embedded_reber_string(max_size)\n",
    "    swap_index=random.randint(0,len(correct)-1)\n",
    "    replacement_char= random.sample(vocab.difference(set(correct[swap_index])),1)[0]\n",
    "    return correct[:swap_index]+replacement_char+correct[swap_index+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1]),\n",
       " ['TTTST', 'TVVST'],\n",
       " [[array([1., 0., 0., 0., 0.]),\n",
       "   array([1., 0., 0., 0., 0.]),\n",
       "   array([1., 0., 0., 0., 0.]),\n",
       "   array([0., 0., 0., 0., 1.]),\n",
       "   array([1., 0., 0., 0., 0.])],\n",
       "  [array([1., 0., 0., 0., 0.]),\n",
       "   array([0., 0., 0., 1., 0.]),\n",
       "   array([0., 0., 0., 1., 0.]),\n",
       "   array([0., 0., 0., 0., 1.]),\n",
       "   array([1., 0., 0., 0., 0.])]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lookup=dict(zip(vocabulary, range(len(vocabulary))))\n",
    "reber_vocabulary_size=len(vocab_lookup)\n",
    "\n",
    "def one_hot_encode(string,vocab_lookup,max_size):\n",
    "    encoded=[np.zeros(len(vocab_lookup)) for _ in range(max_size)]\n",
    "    for i,c in enumerate(string):\n",
    "        encoded[i][vocab_lookup[c]]=1\n",
    "    return encoded\n",
    "    \n",
    "def generate_batch(max_size=30,batch_size=32):\n",
    "    labels=np.random.randint(2, size=[batch_size])\n",
    "    strings=[embedded_reber_string(max_size) if x == 1 else not_reber_string(max_size) for x in labels]\n",
    "    #strings_padded=np.array([word+\"-\"*(max_size-len(word)) for word in strings])\n",
    "    one_hot_string=[one_hot_encode(s,vocab_lookup,max_size) for s in strings]\n",
    "    return labels, strings, one_hot_string\n",
    "\n",
    "generate_batch(batch_size=2,max_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4993 5007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1589, 195, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "labels, words,_= generate_batch(batch_size=10000,max_size=50)\n",
    "nonr=np.array(words)[labels==0]\n",
    "reb=np.array(words)[labels==1]\n",
    "print(len(nonr),len(reb))\n",
    "len(set(nonr)),len(set(reb)),len(set(nonr).intersection(set(reb)))\n",
    "# equal creation, no overlap (good)\n",
    "# when creating wrong string, we get more uniq strings than the real reber strings \n",
    "# 1500 that are valid reber strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning:**\n",
    "- never skip sanity checks in the input data (and in general, unit test:)\n",
    "-> I did not orginally have the sanity check and the non reber strings I created contained valid reber strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReberStringPredictor():\n",
    "    \n",
    "    def __init__(self, vocab_size,num_neurons=100,learning_rate=1e-4,num_layers=1,init=\"he\",\n",
    "                dropout_rate_rnn=0.2,dropout_rate_fc=0.):\n",
    "\n",
    "        # TODO validate params\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_neurons=num_neurons\n",
    "        self.learning_rate=learning_rate\n",
    "        self.num_layers=num_layers\n",
    "        self.init=init\n",
    "        self.dropout_rate_fc=dropout_rate_fc\n",
    "        self.dropout_rate_rnn=dropout_rate_rnn\n",
    "\n",
    "        \n",
    "    def construct_model(self):\n",
    "        reset_graph()\n",
    "\n",
    "        self.X=tf.placeholder(tf.float32,[None,None,self.vocab_size],name=\"inputs\")\n",
    "        self.word_lenghts=tf.reduce_sum(self.X,axis=[1,2],name=\"word_lenghts\")\n",
    "        self.target=tf.placeholder(tf.float32,[None],name=\"targets\")\n",
    "        self.is_training=tf.placeholder_with_default(True,shape=(),name=\"is_training\")\n",
    "        \n",
    "        init=tf.contrib.layers.variance_scaling_initializer() if self.init==\"he\" else tf.contrib.layers.xavier_initializer()\n",
    "        with tf.variable_scope(\"rnn\", initializer=init):\n",
    "            cells=[tf.contrib.rnn.GRUCell(num_units=self.num_neurons,\n",
    "                                                activation=tf.nn.relu) for _ in range(self.num_layers)]\n",
    "        \n",
    "            layers=[tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=tf.cond(self.is_training,\n",
    "                                                lambda: 1-self.dropout_rate_rnn,\n",
    "                                                lambda: 1.0)) for cell in cells]\n",
    "            rnn=tf.contrib.rnn.MultiRNNCell(layers)\n",
    "            self.outputs, self.final_state=tf.nn.dynamic_rnn(rnn,self.X,\n",
    "                                                       sequence_length=self.word_lenghts,\n",
    "                                                       dtype=tf.float32) # 'final_state' is a tensor of shape [batch_size, cell_state_size]\n",
    "\n",
    "        num_outputs=1 # is_rebber_string\n",
    "        logits=tf.layers.dense(self.final_state[-1],num_outputs)\n",
    "        logits=tf.reshape(logits,[-1])\n",
    "        logit_dropped=tf.layers.dropout(logits,self.dropout_rate_fc,training=self.is_training)\n",
    "        \n",
    "        \n",
    "        self.loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.target,logits=logits))\n",
    "\n",
    "        self.training_op= tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # evaluation-only tensors (not needed in training)\n",
    "        self.probability=tf.sigmoid(logits,name=\"y_proba\")\n",
    "        self.y_pred=tf.cast(tf.greater(self.probability,0.5),tf.float32)\n",
    "        self.accuracy=1-tf.reduce_mean(tf.abs(self.y_pred-self.target))\n",
    "    \n",
    "    def get_training_operation(self):\n",
    "        return self.training_op\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1a0169d8d0>:27: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1a0169d8d0>:32: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1a0169d8d0>:35: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/lweichbrodt/bin/miniconda/envs/handson-ml/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/lweichbrodt/bin/miniconda/envs/handson-ml/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/lweichbrodt/bin/miniconda/envs/handson-ml/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/lweichbrodt/bin/miniconda/envs/handson-ml/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1a0169d8d0>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/lweichbrodt/bin/miniconda/envs/handson-ml/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1a0169d8d0>:40: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "original:  ['TTTPT']\n",
      "inputs [[[1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n",
      "input lenghts (without padding) [5.]\n",
      "outputs [[[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.13182531 0.         0.28352454]\n",
      "  [0.08402573 0.         0.23677391]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]]\n",
      "last neural state (array([[0.074564  , 0.        , 0.17452648]], dtype=float32),)\n",
      "Output [0.5751048]\n"
     ]
    }
   ],
   "source": [
    "# visualize single example string\n",
    "max_string_size=10\n",
    "targets, words, one_hot_words=generate_batch(max_size=max_string_size,batch_size=1)\n",
    "m=ReberStringPredictor(vocab_size=len(vocab_lookup),num_neurons=3,num_layers=1)\n",
    "m.construct_model()\n",
    "print(\"original: \",words)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"inputs\",sess.run(m.X, feed_dict={m.X:one_hot_words}))\n",
    "    print(\"input lenghts (without padding)\",sess.run(m.word_lenghts, feed_dict={m.X:one_hot_words})) \n",
    "    print(\"outputs\",sess.run(m.outputs, feed_dict={m.X:one_hot_words}))\n",
    "    print(\"last neural state\",sess.run(m.final_state, feed_dict={m.X:one_hot_words,m.is_training:False}))\n",
    "    print(\"Output\",sess.run(m.probability, feed_dict={m.X:one_hot_words}))\n",
    "\n",
    "# when using different string lenghts the outputs of the padded parts are 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 64\n",
      "dropout 0.0\n",
      "Accuracy:  0.53999996\n",
      "Accuracy:  0.56\n",
      "Accuracy:  0.8\n",
      "Accuracy:  0.95\n",
      "Accuracy:  0.95\n",
      "Accuracy:  1.0\n",
      "Accuracy:  1.0\n",
      "Accuracy:  0.99\n",
      "Accuracy:  1.0\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "max_string_size=50\n",
    "\n",
    "num_training_examples=30000\n",
    "batch_sizes=[64]\n",
    "dropouts=[0.]\n",
    "max_string_size=30\n",
    "for batch_size in batch_sizes:\n",
    "    print(\"batch_size\",batch_size)\n",
    "    for dropout in dropouts:\n",
    "        print(\"dropout\",dropout)\n",
    "        m=ReberStringPredictor(reber_vocabulary_size,num_neurons=30,\n",
    "                   num_layers=1,learning_rate=1e-2,init=\"he\",dropout_rate_fc=dropout,dropout_rate_rnn=dropout)\n",
    "        m.construct_model()\n",
    "        saver=tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            training_op=m.get_training_operation()\n",
    "            for num_batches in range(num_training_examples//batch_size):\n",
    "                targets, words, encoded_words=generate_batch(max_size=max_string_size,batch_size=batch_size)\n",
    "                sess.run([training_op],feed_dict={m.X:encoded_words,m.target:targets})\n",
    "                if num_batches%50==0:\n",
    "                    targets, words, encoded_words=generate_batch(max_size=max_string_size,batch_size=100)\n",
    "                    loss,acc,pr,pred=sess.run([m.loss,m.accuracy,m.probability,m.y_pred],feed_dict={m.X:encoded_words,m.target:targets,\n",
    "                                                                                       m.is_training:False})\n",
    "                    print(\"Accuracy: \", acc)\n",
    "            saver.save(sess,\"./reber_string_predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- get the network to overfit -> I dont get over a acc of 90, why? -> bug in input data (see sanity check)\n",
    "- after fix was implemented we easily get 100% accuracy, even on newly created test/validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final demo on difficult example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples_during_learning=30000\n",
    "valid_reber_strings=[embedded_reber_string() for _ in range(num_examples_during_learning)]\n",
    "\n",
    "gt20=[s for s in valid_reber_strings if len(s)>20]\n",
    "gt27=[s for s in valid_reber_strings if len(s)>27]\n",
    "len(gt20),len(gt27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./reber_string_predictor\n",
      "[0.44921276 0.9995407  0.00312981 0.99518853 0.00112865]\n",
      "Estimated probability that these are Reber strings:\n",
      ": 44.921%\n",
      "TTPPTXXXXXXXXXXXXXVPST: 99.954%\n",
      "TTPPTXXXXXXXXXXXXXVPSS: 0.313%\n",
      "TTPPPPPPPPPPPPPPPTXXXXXXXXXVST: 99.519%\n",
      "TTPPPPPPPPPPPPPPPTXXXXXXXXXVSP: 0.113%\n"
     ]
    }
   ],
   "source": [
    "# idea taken taken from https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb\n",
    "\n",
    "test_strings = [\n",
    "    \"\",\n",
    "    \"TTPPTXXXXXXXXXXXXXVPST\",#22 long\n",
    "    \"TTPPTXXXXXXXXXXXXXVPSS\",\n",
    "    \"TTPPPPPPPPPPPPPPPTXXXXXXXXXVST\",#30 long -> reason: happens too rarely, most likely not in training set\n",
    "    \"TTPPPPPPPPPPPPPPPTXXXXXXXXXVSP\"]\n",
    "X_test = [one_hot_encode(s, vocab_lookup,max_size=38)\n",
    "          for s in test_strings]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./reber_string_predictor\")\n",
    "    y_proba = tf.get_default_graph().get_tensor_by_name(\"y_proba:0\")\n",
    "    inputs = tf.get_default_graph().get_tensor_by_name(\"inputs:0\")\n",
    "    y_proba_val = y_proba.eval(feed_dict={inputs: X_test},session=sess)\n",
    "\n",
    "print(y_proba_val)\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.3f}%\".format(string, 100 * y_proba_val[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning**\n",
    "\n",
    "As usual, looking at examples is useful. \n",
    "- Start from trivial to complex.\n",
    "- Define beforehand your expectations (similar to tests)\n",
    "- define expected behavior before modeling. Example: What should the model do when empty strings are passed (not included in training set)? What is the max string size allowed? (currently: exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handson-ml]",
   "language": "python",
   "name": "conda-env-handson-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
