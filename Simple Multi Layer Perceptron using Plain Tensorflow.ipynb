{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           3,  18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154,\n",
       "         170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "         253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253,\n",
       "         253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253,\n",
       "         253, 205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 # scale image features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=28*28 # mnist format\n",
    "num_hidden=300\n",
    "num_hidden2=100\n",
    "n_outputs=10 # mnist target classes, numbers from 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_tensor=tf.placeholder(shape=[None,num_inputs],dtype=float,name=\"image_input\")\n",
    "    target=tf.placeholder(shape=[None],dtype=tf.int64, name=\"target\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    layer1=tf.layers.dense(input_tensor,num_hidden,activation=\"relu\")\n",
    "    layer2=tf.layers.dense(layer1,num_hidden2,activation=\"relu\")\n",
    "    logits=tf.layers.dense(layer2, n_outputs,activation=None)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target)\n",
    "    loss=tf.reduce_mean(cross_entropy) # mean within batch\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.01)\n",
    "    train_operation=optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"prediction\"):\n",
    "    predicted=tf.argmax(logits,axis=1)  # since softmax uses a logistic function (mononotically increasing) \n",
    "                                        # the argmax of the inputs is the same as the argmax of softmax(inputs) \n",
    "with tf.name_scope(\"evaluation\"):\n",
    "    hit=tf.math.equal(predicted,target)\n",
    "    accuracy=tf.reduce_mean(tf.cast(hit,tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape data for usage\n",
    "train_target=y_train.reshape(-1)\n",
    "test_target=y_test.reshape(-1)\n",
    "train_data=x_train.reshape(x_train.shape[0],-1)\n",
    "test_data=x_test.reshape(x_test.shape[0],-1)\n",
    "train_data.shape,train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0.8889667 0.8968\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "num_epoches=1\n",
    "batch_size=64 # x_train.shape[0]\n",
    "\n",
    "#os.makedirs(\"model/run\",exist_ok=True)\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for e in range(num_epoches):\n",
    "        print(\"Epoch %i\" %e)\n",
    "        for iteration in range(x_train.shape[0]//batch_size):\n",
    "            sample_idx=np.random.randint(0,x_train.shape[0], batch_size)\n",
    "            sess.run(train_operation,feed_dict={target:train_target[sample_idx],\n",
    "                                                  input_tensor:train_data[sample_idx]})\n",
    "        accuracy_training=sess.run(accuracy,feed_dict={target:train_target,\n",
    "                                                      input_tensor:train_data})\n",
    "        accuracy_test=sess.run(accuracy,feed_dict={target:test_target,\n",
    "                                                      input_tensor:test_data})\n",
    "        print(accuracy_training,accuracy_test)\n",
    "    prediction_training=sess.run(predicted,feed_dict={input_tensor:test_data})\n",
    "    saver.save(sess,save_path=\"./model.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load model (for example later, in different environment)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess,\"./model.ckpt\")\n",
    "    \n",
    "    # test that model gives same result in new environment as in training\n",
    "    prediction_serving_environment=sess.run(predicted,feed_dict={input_tensor:test_data}) # use some new data (I reuse test data)\n",
    "    assert np.array_equal(prediction_training,prediction_serving_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'model/run/checkpoint/': Not a directory\r\n"
     ]
    }
   ],
   "source": [
    "! ls model/run/checkpoint/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
